{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0731db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Projects\\pytorchvideo\\pytorchvideo\\data\\frame_video.py:106: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  return [int(c) if c.isdigit() else c for c in re.split(\"(\\d+)\", text)]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from fractions import Fraction\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Optional, Type\n",
    "from math import floor\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorchvideo\n",
    "from pytorchvideo.data.clip_sampling import ClipSampler, make_clip_sampler\n",
    "from pytorchvideo.data.utils import MultiProcessSampler\n",
    "from pytorchvideo.data.video import VideoPathHandler\n",
    "from torch.utils.data import IterableDataset, DataLoader, RandomSampler\n",
    "\n",
    "NUM_LABELS = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ba97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/facebookresearch/pytorchvideo/blob/main/pytorchvideo/data/labeled_video_dataset.py\n",
    "class VideoDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    LabeledVideoDataset handles the storage, loading, decoding and clip sampling for a\n",
    "    video dataset. It assumes each video is stored as either an encoded video\n",
    "    (e.g. mp4, avi) or a frame video (e.g. a folder of jpg, or png)\n",
    "\n",
    "    Note that the label is assigned aftet the clip is sampled\n",
    "    \"\"\"\n",
    "\n",
    "    _MAX_CONSECUTIVE_FAILURES = 10\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        labeled_video_paths: list[tuple[str, Optional[dict]]],\n",
    "        clip_sampler: ClipSampler,\n",
    "        fn2labels: dict,\n",
    "        video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,\n",
    "        transform: Optional[Callable[[dict], Any]] = None,\n",
    "        decode_audio: bool = True,\n",
    "        decode_video: bool = True,\n",
    "        decoder: str = \"pyav\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labeled_video_paths (List[Tuple[str, Optional[dict]]]): List containing\n",
    "                    video file paths and associated labels. If video paths are a folder\n",
    "                    it's interpreted as a frame video, otherwise it must be an encoded\n",
    "                    video.\n",
    "\n",
    "            clip_sampler (ClipSampler): Defines how clips should be sampled from each\n",
    "                video. See the clip sampling documentation for more information.\n",
    "\n",
    "            video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal\n",
    "                video container. This defines the order videos are decoded and,\n",
    "                if necessary, the distributed split.\n",
    "\n",
    "            transform (Callable): This callable is evaluated on the clip output before\n",
    "                the clip is returned. It can be used for user defined preprocessing and\n",
    "                augmentations on the clips. The clip output format is described in __next__().\n",
    "\n",
    "            decode_audio (bool): If True, decode audio from video.\n",
    "\n",
    "            decode_video (bool): If True, decode video frames from a video container.\n",
    "\n",
    "            decoder (str): Defines what type of decoder used to decode a video. Not used for\n",
    "                frame videos.\n",
    "        \"\"\"\n",
    "        self._decode_audio = decode_audio\n",
    "        self._decode_video = decode_video\n",
    "        self._transform = transform\n",
    "        self._clip_sampler = clip_sampler\n",
    "        self._labeled_videos = labeled_video_paths\n",
    "        self._decoder = decoder\n",
    "\n",
    "        # If a RandomSampler is used we need to pass in a custom random generator that\n",
    "        # ensures all PyTorch multiprocess workers have the same random seed.\n",
    "        self._video_random_generator = None\n",
    "        if video_sampler == torch.utils.data.RandomSampler:\n",
    "            self._video_random_generator = torch.Generator()\n",
    "            self._video_sampler = video_sampler(\n",
    "                self._labeled_videos, generator=self._video_random_generator\n",
    "            )\n",
    "        else:\n",
    "            self._video_sampler = video_sampler(self._labeled_videos)\n",
    "\n",
    "        self._video_sampler_iter = None  # Initialized on first call to self.__next__()\n",
    "\n",
    "        # Depending on the clip sampler type, we may want to sample multiple clips\n",
    "        # from one video. In that case, we keep the store video, label and previous sampled\n",
    "        # clip time in these variables.\n",
    "        self._loaded_video_label = None\n",
    "        self._loaded_clip = None\n",
    "        self._last_clip_end_time = Fraction(0, 1)\n",
    "        self.video_path_handler = VideoPathHandler()\n",
    "\n",
    "        # the procedure to load labels is different to LabeledVideoDataset from pytorchvideo\n",
    "        self.fn2labels = fn2labels\n",
    "\n",
    "    @property\n",
    "    def video_sampler(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            The video sampler that defines video sample order. Note that you'll need to\n",
    "            use this property to set the epoch for a torch.utils.data.DistributedSampler.\n",
    "        \"\"\"\n",
    "        return self._video_sampler\n",
    "\n",
    "    @property\n",
    "    def num_videos(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Number of videos in dataset.\n",
    "        \"\"\"\n",
    "        return len(self.video_sampler)\n",
    "\n",
    "    def _get_labels(self, video_info: dict[str, Any]):\n",
    "        start, end, fps, name = video_info[\"clip_start\"], video_info[\"clip_end\"], video_info[\"fps\"], video_info[\"video_name\"]\n",
    "        start_frame, end_frame = floor(start * fps), floor(end * fps)\n",
    "        frame_labels = torch.from_numpy(self.fn2labels[Path(name).stem][start_frame:end_frame])\n",
    "        label = F.one_hot(frame_labels, NUM_LABELS).sum(dim=0).argmax()\n",
    "        return label\n",
    "\n",
    "    def __next__(self) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the next clip based on the clip sampling strategy and video sampler.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with the following format.\n",
    "\n",
    "            .. code-block:: text\n",
    "\n",
    "                {\n",
    "                    'video': <video_tensor>,\n",
    "                    'label': <index_label>,\n",
    "                    'video_label': <index_label>\n",
    "                    'video_index': <video_index>,\n",
    "                    'clip_index': <clip_index>,\n",
    "                    'aug_index': <aug_index>,\n",
    "                }\n",
    "        \"\"\"\n",
    "        if not self._video_sampler_iter:\n",
    "            # Setup MultiProcessSampler here - after PyTorch DataLoader workers are spawned.\n",
    "            self._video_sampler_iter = iter(MultiProcessSampler(self._video_sampler))\n",
    "\n",
    "        for i_try in range(self._MAX_CONSECUTIVE_FAILURES):\n",
    "            # Reuse previously stored video if there are still clips to be sampled from\n",
    "            # the last loaded video.\n",
    "            if self._loaded_video_label:\n",
    "                video, info_dict, video_index = self._loaded_video_label\n",
    "            else:\n",
    "                try:\n",
    "                    video_index = next(self._video_sampler_iter)\n",
    "                except StopIteration:\n",
    "                    self._video_sampler_iter = iter(MultiProcessSampler(self._video_sampler))\n",
    "                    video_index = next(self._video_sampler_iter)\n",
    "                try:\n",
    "                    video_path, info_dict = self._labeled_videos[video_index]\n",
    "                    video = self.video_path_handler.video_from_path(\n",
    "                        video_path,\n",
    "                        decoder=self._decoder,\n",
    "                    )\n",
    "                    self._loaded_video_label = (video, info_dict, video_index)\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        \"Failed to load video with error: {}; trial {}\".format(\n",
    "                            e,\n",
    "                            i_try,\n",
    "                        )\n",
    "                    )\n",
    "                    print(\"Video load exception\")\n",
    "                    continue\n",
    "\n",
    "            video_rate = video._container.streams.video[0].base_rate\n",
    "\n",
    "            (\n",
    "                clip_start,\n",
    "                clip_end,\n",
    "                clip_index,\n",
    "                aug_index,\n",
    "                is_last_clip,\n",
    "            ) = self._clip_sampler(self._last_clip_end_time, video.duration, info_dict)\n",
    "\n",
    "            if isinstance(clip_start, list):  # multi-clip in each sample\n",
    "                # Only load the clips once and reuse previously stored clips if there are multiple\n",
    "                # views for augmentations to perform on the same clips.\n",
    "                if aug_index[0] == 0:\n",
    "                    self._loaded_clip = {}\n",
    "                    loaded_clip_list = []\n",
    "                    for i in range(len(clip_start)):\n",
    "                        clip_dict = video.get_clip(clip_start[i], clip_end[i])\n",
    "                        if clip_dict is None or clip_dict[\"video\"] is None:\n",
    "                            self._loaded_clip = None\n",
    "                            break\n",
    "                        loaded_clip_list.append(clip_dict)\n",
    "\n",
    "                    if self._loaded_clip is not None:\n",
    "                        for key in loaded_clip_list[0].keys():\n",
    "                            self._loaded_clip[key] = [x[key] for x in loaded_clip_list]\n",
    "\n",
    "            else:  # single clip case\n",
    "                # Only load the clip once and reuse previously stored clip if there are multiple\n",
    "                # views for augmentations to perform on the same clip.\n",
    "                if aug_index == 0:\n",
    "                    self._loaded_clip = video.get_clip(clip_start, clip_end)\n",
    "\n",
    "            self._last_clip_end_time = clip_end\n",
    "\n",
    "            video_is_null = (\n",
    "                self._loaded_clip is None or self._loaded_clip[\"video\"] is None\n",
    "            )\n",
    "            if (\n",
    "                is_last_clip[-1] if isinstance(is_last_clip, list) else is_last_clip\n",
    "            ) or video_is_null:\n",
    "                # Close the loaded encoded video and reset the last sampled clip time ready\n",
    "                # to sample a new video on the next iteration.\n",
    "                self._loaded_video_label[0].close()\n",
    "                self._loaded_video_label = None\n",
    "                self._last_clip_end_time = None\n",
    "                self._clip_sampler.reset()\n",
    "\n",
    "                # Force garbage collection to release video container immediately\n",
    "                # otherwise memory can spike.\n",
    "                gc.collect()\n",
    "\n",
    "                if video_is_null:\n",
    "                    print(\n",
    "                        \"Failed to load clip {}; trial {}\".format(video.name, i_try)\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            info_dict[\"label\"] = self._get_labels({\n",
    "                \"clip_start\": clip_start.numerator,\n",
    "                \"clip_end\": clip_end.numerator,\n",
    "                \"video_name\": video.name,\n",
    "                \"fps\": video_rate,\n",
    "            })\n",
    "\n",
    "            frames = self._loaded_clip[\"video\"]\n",
    "            audio_samples = self._loaded_clip[\"audio\"]\n",
    "            sample_dict = {\n",
    "                \"video\": frames,\n",
    "                \"video_name\": video.name,\n",
    "                \"video_index\": video_index,\n",
    "                \"clip_index\": clip_index,\n",
    "                \"aug_index\": aug_index,\n",
    "                **info_dict,\n",
    "                **({\"audio\": audio_samples} if audio_samples is not None else {}),\n",
    "            }\n",
    "            if self._transform is not None:\n",
    "                sample_dict = self._transform(sample_dict)\n",
    "\n",
    "                # User can force dataset to continue by returning None in transform.\n",
    "                if sample_dict is None:\n",
    "                    continue\n",
    "\n",
    "            return sample_dict\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to load video after {self._MAX_CONSECUTIVE_FAILURES} retries.\"\n",
    "            )\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._video_sampler_iter = None  # Reset video sampler\n",
    "\n",
    "        # If we're in a PyTorch DataLoader multiprocessing context, we need to use the\n",
    "        # same seed for each worker's RandomSampler generator. The workers at each\n",
    "        # __iter__ call are created from the unique value: worker_info.seed - worker_info.id,\n",
    "        # which we can use for this seed.\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if self._video_random_generator is not None and worker_info is not None:\n",
    "            base_seed = worker_info.seed - worker_info.id\n",
    "            self._video_random_generator.manual_seed(base_seed)\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2466f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = Path(\"dataset\") / \"videos\" / \"mp4\"\n",
    "\n",
    "video_paths = [(video_dir / path, {'label': None}) for path in os.listdir(video_dir)]\n",
    "filename_to_labels = dict()\n",
    "with np.load(Path(\"dataset\") / \"annotations_processed.npz\") as npz_file:\n",
    "    for key in npz_file.files:\n",
    "        filename_to_labels[key] = npz_file[key]\n",
    "train_dataset = VideoDataset(\n",
    "    labeled_video_paths=video_paths,\n",
    "    clip_sampler=make_clip_sampler(\"random\", 2),\n",
    "    fn2labels=filename_to_labels,\n",
    "    video_sampler=partial(RandomSampler, replacement=True)\n",
    ")\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, num_workers=2)\n",
    "# train_dataloader_iterator = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721a2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b79d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.models import ResNetBasicHead\n",
    "from torch import nn\n",
    "from dataset import create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15029e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\admin/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "model_name = \"slowfast_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a878ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks[6].proj = nn.Linear(2304, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b44bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef965735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 33])\n",
      "torch.Size([2, 33])\n"
     ]
    }
   ],
   "source": [
    "out = model([torch.randn(2, 3, 8, 224, 224), torch.randn(2, 3, 32, 224, 224)])\n",
    "for item in out:\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b20e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e19a24ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 8, 256, 256]), torch.Size([4, 3, 32, 256, 256]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['video'][0].shape, batch['video'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4517f17d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import create_dataset\n",
    "\n",
    "train, val = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b3480",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'fractions.Fraction'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    169\u001b[39m clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m clone.update(\n\u001b[32m    171\u001b[39m     {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m         key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m     }\n\u001b[32m    177\u001b[39m )\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    235\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    236\u001b[39m                 collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    237\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    238\u001b[39m             ]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format.format(elem_type))\n",
      "\u001b[31mTypeError\u001b[39m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'fractions.Fraction'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:43\u001b[39m, in \u001b[36m_IterableDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     42\u001b[39m     data = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset_iter)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:192\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    180\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[32m    181\u001b[39m                 {\n\u001b[32m    182\u001b[39m                     key: collate(\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m                 }\n\u001b[32m    187\u001b[39m             )\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    189\u001b[39m         \u001b[38;5;66;03m# The mapping type may not support `copy()` / `update(mapping)`\u001b[39;00m\n\u001b[32m    190\u001b[39m         \u001b[38;5;66;03m# or `__init__(iterable)`.\u001b[39;00m\n\u001b[32m    191\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    194\u001b[39m         }\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[33m\"\u001b[39m\u001b[33m_fields\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[32m    197\u001b[39m         *(\n\u001b[32m    198\u001b[39m             collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    199\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*batch)\n\u001b[32m    200\u001b[39m         )\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniforge3\\envs\\compvis\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    233\u001b[39m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[32m    234\u001b[39m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[32m    235\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    236\u001b[39m                 collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    237\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    238\u001b[39m             ]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format.format(elem_type))\n",
      "\u001b[31mTypeError\u001b[39m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'fractions.Fraction'>"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in val:\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be2e065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
