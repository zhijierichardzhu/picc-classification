{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0731db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from pytorchvideo.data.video import VideoPathHandler\n",
    "from pytorchvideo.data.clip_sampling import make_clip_sampler, ClipSampler\n",
    "import os\n",
    "from pytorchvideo.data.labeled_video_dataset import labeled_video_dataset\n",
    "from pytorchvideo.data.utils import MultiProcessSampler\n",
    "from fractions import Fraction\n",
    "from torch.utils.data import IterableDataset\n",
    "from typing import Optional, Type, Callable, Any\n",
    "import gc\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03ba97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/facebookresearch/pytorchvideo/blob/main/pytorchvideo/data/labeled_video_dataset.py\n",
    "class VideoDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"\n",
    "    LabeledVideoDataset handles the storage, loading, decoding and clip sampling for a\n",
    "    video dataset. It assumes each video is stored as either an encoded video\n",
    "    (e.g. mp4, avi) or a frame video (e.g. a folder of jpg, or png)\n",
    "\n",
    "    Note that the label is assigned aftet the clip is sampled\n",
    "    \"\"\"\n",
    "\n",
    "    _MAX_CONSECUTIVE_FAILURES = 10\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        labeled_video_paths: list[tuple[str, Optional[dict]]],\n",
    "        clip_sampler: ClipSampler,\n",
    "        labels_npz_path: Path | str,\n",
    "        video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,\n",
    "        transform: Optional[Callable[[dict], Any]] = None,\n",
    "        decode_audio: bool = True,\n",
    "        decode_video: bool = True,\n",
    "        decoder: str = \"pyav\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labeled_video_paths (List[Tuple[str, Optional[dict]]]): List containing\n",
    "                    video file paths and associated labels. If video paths are a folder\n",
    "                    it's interpreted as a frame video, otherwise it must be an encoded\n",
    "                    video.\n",
    "\n",
    "            clip_sampler (ClipSampler): Defines how clips should be sampled from each\n",
    "                video. See the clip sampling documentation for more information.\n",
    "\n",
    "            video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal\n",
    "                video container. This defines the order videos are decoded and,\n",
    "                if necessary, the distributed split.\n",
    "\n",
    "            transform (Callable): This callable is evaluated on the clip output before\n",
    "                the clip is returned. It can be used for user defined preprocessing and\n",
    "                augmentations on the clips. The clip output format is described in __next__().\n",
    "\n",
    "            decode_audio (bool): If True, decode audio from video.\n",
    "\n",
    "            decode_video (bool): If True, decode video frames from a video container.\n",
    "\n",
    "            decoder (str): Defines what type of decoder used to decode a video. Not used for\n",
    "                frame videos.\n",
    "        \"\"\"\n",
    "        self._decode_audio = decode_audio\n",
    "        self._decode_video = decode_video\n",
    "        self._transform = transform\n",
    "        self._clip_sampler = clip_sampler\n",
    "        self._labeled_videos = labeled_video_paths\n",
    "        self._decoder = decoder\n",
    "\n",
    "        # If a RandomSampler is used we need to pass in a custom random generator that\n",
    "        # ensures all PyTorch multiprocess workers have the same random seed.\n",
    "        self._video_random_generator = None\n",
    "        if video_sampler == torch.utils.data.RandomSampler:\n",
    "            self._video_random_generator = torch.Generator()\n",
    "            self._video_sampler = video_sampler(\n",
    "                self._labeled_videos, generator=self._video_random_generator\n",
    "            )\n",
    "        else:\n",
    "            self._video_sampler = video_sampler(self._labeled_videos)\n",
    "\n",
    "        self._video_sampler_iter = None  # Initialized on first call to self.__next__()\n",
    "\n",
    "        # Depending on the clip sampler type, we may want to sample multiple clips\n",
    "        # from one video. In that case, we keep the store video, label and previous sampled\n",
    "        # clip time in these variables.\n",
    "        self._loaded_video_label = None\n",
    "        self._loaded_clip = None\n",
    "        self._last_clip_end_time = Fraction(0, 1)\n",
    "        self.video_path_handler = VideoPathHandler()\n",
    "\n",
    "        # the procedure to load labels is different to LabeledVideoDataset from pytorchvideo\n",
    "        self.labels_npz = np.load(labels_npz_path)\n",
    "\n",
    "    @property\n",
    "    def video_sampler(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            The video sampler that defines video sample order. Note that you'll need to\n",
    "            use this property to set the epoch for a torch.utils.data.DistributedSampler.\n",
    "        \"\"\"\n",
    "        return self._video_sampler\n",
    "\n",
    "    @property\n",
    "    def num_videos(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Number of videos in dataset.\n",
    "        \"\"\"\n",
    "        return len(self.video_sampler)\n",
    "    \n",
    "    def _get_labels(self, video_info: dict[str, Any]):\n",
    "        start, end, fps, name = video_info[\"clip_start\"], video_info[\"clip_end\"], video_info[\"fps\"], video_info[\"video_name\"]\n",
    "        frame_labels = torch.from_numpy(self.labels_npz[Path(name).stem][floor(start * fps):floor(end * fps)])\n",
    "        label = F.one_hot(frame_labels, 32).sum(dim=0).argmax()\n",
    "        return label\n",
    "\n",
    "    def __next__(self) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the next clip based on the clip sampling strategy and video sampler.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with the following format.\n",
    "\n",
    "            .. code-block:: text\n",
    "\n",
    "                {\n",
    "                    'video': <video_tensor>,\n",
    "                    'label': <index_label>,\n",
    "                    'video_label': <index_label>\n",
    "                    'video_index': <video_index>,\n",
    "                    'clip_index': <clip_index>,\n",
    "                    'aug_index': <aug_index>,\n",
    "                }\n",
    "        \"\"\"\n",
    "        if not self._video_sampler_iter:\n",
    "            # Setup MultiProcessSampler here - after PyTorch DataLoader workers are spawned.\n",
    "            self._video_sampler_iter = iter(MultiProcessSampler(self._video_sampler))\n",
    "\n",
    "        for i_try in range(self._MAX_CONSECUTIVE_FAILURES):\n",
    "            # Reuse previously stored video if there are still clips to be sampled from\n",
    "            # the last loaded video.\n",
    "            if self._loaded_video_label:\n",
    "                video, info_dict, video_index = self._loaded_video_label\n",
    "            else:\n",
    "                video_index = next(self._video_sampler_iter)\n",
    "                try:\n",
    "                    video_path, info_dict = self._labeled_videos[video_index]\n",
    "                    video = self.video_path_handler.video_from_path(\n",
    "                        video_path,\n",
    "                        decoder=self._decoder,\n",
    "                    )\n",
    "                    self._loaded_video_label = (video, info_dict, video_index)\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        \"Failed to load video with error: {}; trial {}\".format(\n",
    "                            e,\n",
    "                            i_try,\n",
    "                        )\n",
    "                    )\n",
    "                    print(\"Video load exception\")\n",
    "                    continue\n",
    "\n",
    "            video_fps = video._container.streams.video[0].base_rate\n",
    "\n",
    "            (\n",
    "                clip_start,\n",
    "                clip_end,\n",
    "                clip_index,\n",
    "                aug_index,\n",
    "                is_last_clip,\n",
    "            ) = self._clip_sampler(self._last_clip_end_time, video.duration, info_dict)\n",
    "\n",
    "            if isinstance(clip_start, list):  # multi-clip in each sample\n",
    "                # Only load the clips once and reuse previously stored clips if there are multiple\n",
    "                # views for augmentations to perform on the same clips.\n",
    "                if aug_index[0] == 0:\n",
    "                    self._loaded_clip = {}\n",
    "                    loaded_clip_list = []\n",
    "                    for i in range(len(clip_start)):\n",
    "                        clip_dict = video.get_clip(clip_start[i], clip_end[i])\n",
    "                        if clip_dict is None or clip_dict[\"video\"] is None:\n",
    "                            self._loaded_clip = None\n",
    "                            break\n",
    "                        loaded_clip_list.append(clip_dict)\n",
    "\n",
    "                    if self._loaded_clip is not None:\n",
    "                        for key in loaded_clip_list[0].keys():\n",
    "                            self._loaded_clip[key] = [x[key] for x in loaded_clip_list]\n",
    "\n",
    "            else:  # single clip case\n",
    "                # Only load the clip once and reuse previously stored clip if there are multiple\n",
    "                # views for augmentations to perform on the same clip.\n",
    "                if aug_index == 0:\n",
    "                    self._loaded_clip = video.get_clip(clip_start, clip_end)\n",
    "\n",
    "            self._last_clip_end_time = clip_end\n",
    "\n",
    "            video_is_null = (\n",
    "                self._loaded_clip is None or self._loaded_clip[\"video\"] is None\n",
    "            )\n",
    "            if (\n",
    "                is_last_clip[-1] if isinstance(is_last_clip, list) else is_last_clip\n",
    "            ) or video_is_null:\n",
    "                # Close the loaded encoded video and reset the last sampled clip time ready\n",
    "                # to sample a new video on the next iteration.\n",
    "                self._loaded_video_label[0].close()\n",
    "                self._loaded_video_label = None\n",
    "                self._last_clip_end_time = None\n",
    "                self._clip_sampler.reset()\n",
    "\n",
    "                # Force garbage collection to release video container immediately\n",
    "                # otherwise memory can spike.\n",
    "                gc.collect()\n",
    "\n",
    "                if video_is_null:\n",
    "                    print(\n",
    "                        \"Failed to load clip {}; trial {}\".format(video.name, i_try)\n",
    "                    )\n",
    "                    continue\n",
    "            \n",
    "            info_dict[\"label\"] = self._get_labels({\n",
    "                \"clip_start\": clip_start,\n",
    "                \"clip_end\": clip_end,\n",
    "                \"video_name\": video.name,\n",
    "                \"fps\": video_fps,\n",
    "            })\n",
    "\n",
    "            frames = self._loaded_clip[\"video\"]\n",
    "            audio_samples = self._loaded_clip[\"audio\"]\n",
    "            sample_dict = {\n",
    "                \"video\": frames,\n",
    "                \"video_name\": video.name,\n",
    "                \"video_index\": video_index,\n",
    "                \"clip_index\": clip_index,\n",
    "                \"aug_index\": aug_index,\n",
    "                **info_dict,\n",
    "                **({\"audio\": audio_samples} if audio_samples is not None else {}),\n",
    "            }\n",
    "            if self._transform is not None:\n",
    "                sample_dict = self._transform(sample_dict)\n",
    "\n",
    "                # User can force dataset to continue by returning None in transform.\n",
    "                if sample_dict is None:\n",
    "                    continue\n",
    "\n",
    "            return sample_dict\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to load video after {self._MAX_CONSECUTIVE_FAILURES} retries.\"\n",
    "            )\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._video_sampler_iter = None  # Reset video sampler\n",
    "\n",
    "        # If we're in a PyTorch DataLoader multiprocessing context, we need to use the\n",
    "        # same seed for each worker's RandomSampler generator. The workers at each\n",
    "        # __iter__ call are created from the unique value: worker_info.seed - worker_info.id,\n",
    "        # which we can use for this seed.\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if self._video_random_generator is not None and worker_info is not None:\n",
    "            base_seed = worker_info.seed - worker_info.id\n",
    "            self._video_random_generator.manual_seed(base_seed)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f2466f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = Path(\"dataset\") / \"videos\" / \"mp4\"\n",
    "\n",
    "video_paths = [(video_dir / path, {'label': None}) for path in os.listdir(video_dir)]\n",
    "dataset = VideoDataset(\n",
    "    labeled_video_paths=video_paths,\n",
    "    clip_sampler=make_clip_sampler(\"random\", 2),\n",
    "    labels_npz_path=Path(\"dataset\") / \"annotations_processed.npz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "210f4f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fraction(230590403373107, 549755813888) < 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2d4b3005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': tensor([[[[ 25.,  25.,  25.,  ..., 109., 109., 109.],\n",
       "           [ 25.,  25.,  25.,  ..., 108., 108., 108.],\n",
       "           [ 28.,  28.,  27.,  ..., 113., 115., 113.],\n",
       "           ...,\n",
       "           [ 44.,  44.,  44.,  ...,  97.,  97.,  97.],\n",
       "           [ 44.,  44.,  44.,  ...,  97.,  97.,  97.],\n",
       "           [ 44.,  44.,  44.,  ...,  97.,  97.,  97.]],\n",
       " \n",
       "          [[ 25.,  25.,  25.,  ..., 111., 111., 111.],\n",
       "           [ 25.,  25.,  26.,  ..., 113., 115., 113.],\n",
       "           [ 27.,  27.,  28.,  ..., 113., 112., 113.],\n",
       "           ...,\n",
       "           [ 44.,  44.,  44.,  ...,  97.,  97.,  97.],\n",
       "           [ 44.,  44.,  44.,  ...,  97.,  97.,  97.],\n",
       "           [ 44.,  44.,  44.,  ...,  97.,  97.,  97.]],\n",
       " \n",
       "          [[ 24.,  23.,  21.,  ..., 116., 116., 116.],\n",
       "           [ 25.,  24.,  23.,  ..., 117., 117., 117.],\n",
       "           [ 25.,  24.,  20.,  ..., 119., 119., 119.],\n",
       "           ...,\n",
       "           [ 44.,  44.,  44.,  ...,  97.,  97.,  97.],\n",
       "           [ 44.,  44.,  44.,  ...,  97.,  97.,  97.],\n",
       "           [ 44.,  44.,  44.,  ...,  97.,  97.,  97.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 62.,  62.,  62.,  ...,  78.,  78.,  78.],\n",
       "           [ 61.,  61.,  61.,  ...,  78.,  78.,  78.],\n",
       "           [ 61.,  61.,  61.,  ...,  78.,  78.,  78.],\n",
       "           ...,\n",
       "           [ 31.,  31.,  31.,  ...,  72.,  72.,  72.],\n",
       "           [ 31.,  31.,  31.,  ...,  72.,  72.,  72.],\n",
       "           [ 31.,  31.,  31.,  ...,  72.,  72.,  72.]],\n",
       " \n",
       "          [[ 62.,  62.,  62.,  ...,  78.,  78.,  78.],\n",
       "           [ 61.,  61.,  61.,  ...,  78.,  78.,  78.],\n",
       "           [ 61.,  61.,  61.,  ...,  78.,  78.,  78.],\n",
       "           ...,\n",
       "           [ 34.,  34.,  34.,  ...,  73.,  73.,  73.],\n",
       "           [ 34.,  34.,  34.,  ...,  73.,  73.,  73.],\n",
       "           [ 32.,  32.,  32.,  ...,  73.,  73.,  73.]],\n",
       " \n",
       "          [[ 61.,  61.,  61.,  ...,  78.,  78.,  78.],\n",
       "           [ 61.,  61.,  61.,  ...,  78.,  78.,  78.],\n",
       "           [ 61.,  61.,  61.,  ...,  78.,  78.,  78.],\n",
       "           ...,\n",
       "           [ 34.,  34.,  34.,  ...,  73.,  73.,  73.],\n",
       "           [ 32.,  32.,  32.,  ...,  73.,  73.,  73.],\n",
       "           [ 32.,  32.,  32.,  ...,  73.,  73.,  73.]]],\n",
       " \n",
       " \n",
       "         [[[ 23.,  23.,  23.,  ..., 129., 129., 129.],\n",
       "           [ 23.,  23.,  23.,  ..., 128., 128., 128.],\n",
       "           [ 23.,  23.,  22.,  ..., 130., 132., 130.],\n",
       "           ...,\n",
       "           [ 62.,  62.,  62.,  ...,  92.,  92.,  92.],\n",
       "           [ 62.,  62.,  62.,  ...,  92.,  92.,  92.],\n",
       "           [ 62.,  62.,  62.,  ...,  92.,  92.,  92.]],\n",
       " \n",
       "          [[ 23.,  23.,  23.,  ..., 128., 128., 128.],\n",
       "           [ 23.,  23.,  24.,  ..., 130., 132., 130.],\n",
       "           [ 22.,  22.,  23.,  ..., 128., 127., 128.],\n",
       "           ...,\n",
       "           [ 62.,  62.,  62.,  ...,  92.,  92.,  92.],\n",
       "           [ 62.,  62.,  62.,  ...,  92.,  92.,  92.],\n",
       "           [ 62.,  62.,  62.,  ...,  92.,  92.,  92.]],\n",
       " \n",
       "          [[ 22.,  21.,  19.,  ..., 131., 131., 131.],\n",
       "           [ 23.,  22.,  21.,  ..., 132., 132., 132.],\n",
       "           [ 23.,  22.,  18.,  ..., 131., 131., 131.],\n",
       "           ...,\n",
       "           [ 62.,  62.,  62.,  ...,  92.,  92.,  92.],\n",
       "           [ 62.,  62.,  62.,  ...,  92.,  92.,  92.],\n",
       "           [ 62.,  62.,  62.,  ...,  92.,  92.,  92.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 66.,  66.,  66.,  ...,  74.,  74.,  74.],\n",
       "           [ 65.,  65.,  65.,  ...,  74.,  74.,  74.],\n",
       "           [ 65.,  65.,  65.,  ...,  74.,  74.,  74.],\n",
       "           ...,\n",
       "           [ 31.,  31.,  31.,  ...,  65.,  65.,  65.],\n",
       "           [ 31.,  31.,  31.,  ...,  65.,  65.,  65.],\n",
       "           [ 31.,  31.,  31.,  ...,  65.,  65.,  65.]],\n",
       " \n",
       "          [[ 66.,  66.,  66.,  ...,  74.,  74.,  74.],\n",
       "           [ 65.,  65.,  65.,  ...,  74.,  74.,  74.],\n",
       "           [ 65.,  65.,  65.,  ...,  74.,  74.,  74.],\n",
       "           ...,\n",
       "           [ 35.,  35.,  35.,  ...,  66.,  66.,  66.],\n",
       "           [ 35.,  35.,  35.,  ...,  66.,  66.,  66.],\n",
       "           [ 33.,  33.,  33.,  ...,  66.,  66.,  66.]],\n",
       " \n",
       "          [[ 65.,  65.,  65.,  ...,  74.,  74.,  74.],\n",
       "           [ 65.,  65.,  65.,  ...,  74.,  74.,  74.],\n",
       "           [ 65.,  65.,  65.,  ...,  74.,  74.,  74.],\n",
       "           ...,\n",
       "           [ 35.,  35.,  35.,  ...,  66.,  66.,  66.],\n",
       "           [ 33.,  33.,  33.,  ...,  66.,  66.,  66.],\n",
       "           [ 33.,  33.,  33.,  ...,  66.,  66.,  66.]]],\n",
       " \n",
       " \n",
       "         [[[ 24.,  24.,  24.,  ..., 134., 134., 134.],\n",
       "           [ 24.,  24.,  24.,  ..., 133., 133., 133.],\n",
       "           [ 25.,  25.,  24.,  ..., 136., 138., 136.],\n",
       "           ...,\n",
       "           [ 61.,  61.,  61.,  ...,  62.,  62.,  62.],\n",
       "           [ 61.,  61.,  61.,  ...,  62.,  62.,  62.],\n",
       "           [ 61.,  61.,  61.,  ...,  62.,  62.,  62.]],\n",
       " \n",
       "          [[ 24.,  24.,  24.,  ..., 134., 134., 134.],\n",
       "           [ 24.,  24.,  25.,  ..., 136., 138., 136.],\n",
       "           [ 24.,  24.,  25.,  ..., 133., 132., 133.],\n",
       "           ...,\n",
       "           [ 61.,  61.,  61.,  ...,  62.,  62.,  62.],\n",
       "           [ 61.,  61.,  61.,  ...,  62.,  62.,  62.],\n",
       "           [ 61.,  61.,  61.,  ...,  62.,  62.,  62.]],\n",
       " \n",
       "          [[ 20.,  19.,  17.,  ..., 136., 136., 136.],\n",
       "           [ 21.,  20.,  19.,  ..., 137., 137., 137.],\n",
       "           [ 24.,  23.,  19.,  ..., 135., 135., 135.],\n",
       "           ...,\n",
       "           [ 61.,  61.,  61.,  ...,  62.,  62.,  62.],\n",
       "           [ 61.,  61.,  61.,  ...,  62.,  62.,  62.],\n",
       "           [ 61.,  61.,  61.,  ...,  62.,  62.,  62.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 70.,  70.,  70.,  ...,  57.,  57.,  57.],\n",
       "           [ 69.,  69.,  69.,  ...,  57.,  57.,  57.],\n",
       "           [ 69.,  69.,  69.,  ...,  57.,  57.,  57.],\n",
       "           ...,\n",
       "           [ 39.,  39.,  39.,  ...,  47.,  47.,  47.],\n",
       "           [ 39.,  39.,  39.,  ...,  47.,  47.,  47.],\n",
       "           [ 39.,  39.,  39.,  ...,  47.,  47.,  47.]],\n",
       " \n",
       "          [[ 70.,  70.,  70.,  ...,  57.,  57.,  57.],\n",
       "           [ 69.,  69.,  69.,  ...,  57.,  57.,  57.],\n",
       "           [ 69.,  69.,  69.,  ...,  57.,  57.,  57.],\n",
       "           ...,\n",
       "           [ 40.,  40.,  40.,  ...,  48.,  48.,  48.],\n",
       "           [ 40.,  40.,  40.,  ...,  48.,  48.,  48.],\n",
       "           [ 38.,  38.,  38.,  ...,  48.,  48.,  48.]],\n",
       " \n",
       "          [[ 69.,  69.,  69.,  ...,  57.,  57.,  57.],\n",
       "           [ 69.,  69.,  69.,  ...,  57.,  57.,  57.],\n",
       "           [ 69.,  69.,  69.,  ...,  57.,  57.,  57.],\n",
       "           ...,\n",
       "           [ 40.,  40.,  40.,  ...,  48.,  48.,  48.],\n",
       "           [ 38.,  38.,  38.,  ...,  48.,  48.,  48.],\n",
       "           [ 38.,  38.,  38.,  ...,  48.,  48.,  48.]]]]),\n",
       " 'video_name': '2025-05-30_14-45-17.mp4',\n",
       " 'video_index': 4,\n",
       " 'clip_index': 0,\n",
       " 'aug_index': 0,\n",
       " 'label': tensor(13)}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = next(dataset)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692db512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "video_pyav = out[\"raw_video\"]._container.streams.video[0]\n",
    "labels_npz = np.load(Path(\"dataset\") / \"annotations_processed.npz\")\n",
    "# clip_start, clip_end = out[\"clip_start\"][0], out[\"clip_start\"][1]\n",
    "clip_start, clip_end = 0, 1\n",
    "print(clip_start, clip_end)\n",
    "frame_labels = torch.from_numpy(labels_npz[Path(out['video_name']).stem][clip_start * 30:clip_end * 30])\n",
    "F.one_hot(frame_labels, 32).sum(dim=0).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95eb83f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_pyav = out[\"raw_video\"]._container.streams.video[0]\n",
    "video_pyav.base_rate.numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a19363fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(Path(\"dataset\") / \"videos\" / \"mp4\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c4dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = VideoPathHandler()\n",
    "clip_sampler = make_clip_sampler(\"uniform\", 2)\n",
    "encoded_video = handler.video_from_path(os.path.join(\"dataset\", \"videos\", \"mp4\", \"2025-06-03_14-04-34.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea32a907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 60, 1080, 1920])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = encoded_video.get_clip(Fraction(0, 1), Fraction(2, 1))\n",
    "results[\"video\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b6f590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClipInfo(clip_start_sec=Fraction(0, 1), clip_end_sec=Fraction(2, 1), clip_index=1, aug_index=0, is_last_clip=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_result = clip_sampler(0, encoded_video.duration, {})\n",
    "sample_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786ab48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClipInfo(clip_start_sec=Fraction(0, 1), clip_end_sec=Fraction(1, 2), clip_index=8, aug_index=0, is_last_clip=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    clip_start,\n",
    "    clip_end,\n",
    "    clip_index,\n",
    "    aug_index,\n",
    "    is_last_clip,\n",
    ") = self._clip_sampler(self._last_clip_end_time, video.duration, info_dict)\n",
    "\n",
    "if isinstance(clip_start, list):  # multi-clip in each sample\n",
    "    # Only load the clips once and reuse previously stored clips if there are multiple\n",
    "    # views for augmentations to perform on the same clips.\n",
    "    if aug_index[0] == 0:\n",
    "        self._loaded_clip = {}\n",
    "        loaded_clip_list = []\n",
    "        for i in range(len(clip_start)):\n",
    "            clip_dict = video.get_clip(clip_start[i], clip_end[i])\n",
    "            if clip_dict is None or clip_dict[\"video\"] is None:\n",
    "                self._loaded_clip = None\n",
    "                break\n",
    "            loaded_clip_list.append(clip_dict)\n",
    "\n",
    "        if self._loaded_clip is not None:\n",
    "            for key in loaded_clip_list[0].keys():\n",
    "                self._loaded_clip[key] = [x[key] for x in loaded_clip_list]\n",
    "\n",
    "else:  # single clip case\n",
    "    # Only load the clip once and reuse previously stored clip if there are multiple\n",
    "    # views for augmentations to perform on the same clip.\n",
    "    if aug_index == 0:\n",
    "        self._loaded_clip = video.get_clip(clip_start, clip_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850362c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vid = av.open(Path(\"dataset\") / \"videos\" / \"2025-05-29_17-05-39.mkv\", mode=\"r\")\n",
    "\n",
    "video_stream = input_vid.streams.video[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7a91b109",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvideo_stream\u001b[49m.base_rate\n",
      "\u001b[31mNameError\u001b[39m: name 'video_stream' is not defined"
     ]
    }
   ],
   "source": [
    "video_stream.base_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc29ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1bcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3.0.211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15693e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2215b162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2025-06-03_14-18-49'}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_set = set([Path(fn).stem for fn in os.listdir(Path(\"dataset\") / \"annotations\")])\n",
    "video_set = set([Path(fn).stem for fn in os.listdir(Path(\"dataset\") / \"videos\" / \"mp4\")])\n",
    "label_set - video_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f1a27f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2025-06-03_14-18-49'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_set - video_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0552cc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2025-06-03_14-18-49-3'}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_set - label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf4580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
